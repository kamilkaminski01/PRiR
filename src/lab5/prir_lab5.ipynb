{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPUbNAI3Pjc+gUy63VBCIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilkaminski01/PRiR/blob/master/src/lab5/prir_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Programowanie Równoległe i Rozproszone**\n",
        "# Kamil Kamiński LAB gr.1"
      ],
      "metadata": {
        "id": "B9BhBmXDEHDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Laboratorium nr 5**"
      ],
      "metadata": {
        "id": "GMPFnftKEX2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie 1** - Zaimplementuj w środowisku MPI obliczanie równoległe liczby PI z wzoru Leibniz-a.\n",
        "\n",
        "Proces 0, to proces Root, który zarządza obliczeniami:\n",
        "- wysyła do procesów roboczych Slave, indeksy wyrazów szeregu poczatek_local i koniec_local, z których należy obliczyć wyniki cząstkowe\n",
        "- zbiera wyniki cząstkowe z procesów i wyświetla wynik przybliżenia PI"
      ],
      "metadata": {
        "id": "51vVLNLCv-pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "cat > pi-mpi.c << EOF\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int rank;\n",
        "    int num_proc;\n",
        "    int tag = 1;\n",
        "    float wynik = 0;\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n",
        "    MPI_Status status;\n",
        "\n",
        "    double start = MPI_Wtime();\n",
        "    if(rank!=0)\n",
        "    {\n",
        "        MPI_Recv(&wynik, 1, MPI_FLOAT, rank-1, tag, MPI_COMM_WORLD, &status);\n",
        "    }\n",
        "\n",
        "    // Wzor Leibniza\n",
        "    wynik = wynik + powf(-1, rank) / (2 * (rank+1) - 1) * 4;\n",
        "\n",
        "    if(rank != num_proc-1)\n",
        "    {\n",
        "        MPI_Send(&wynik, 1, MPI_FLOAT, rank+1, tag, MPI_COMM_WORLD);\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        double end = MPI_Wtime();\n",
        "        printf(\"Przyblizenie PI = %f\\n\", wynik);\n",
        "        printf(\"Czas obliczen %f\",end - start);\n",
        "    }\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "EOF\n",
        "mpicc pi-mpi.c -lm && mpirun -n 22 --allow-run-as-root a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEjFDM_o_ZJO",
        "outputId": "30d3d4e6-fd28-4aa8-af8a-d20d86793709"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Przyblizenie PI = 3.096161\n",
            "Czas obliczen 0.010571"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie 2** - Zaimplementuj w środowisku MPI wyznaczenie numerycznej wartość całki y=f(x) (postać funkcji wybierasz sam) w przedziale <a,b> przy pomocy N trapezów.\n",
        "\n",
        "Proces 0, to proces Root, który zarządza obliczeniami:\n",
        "- wysyła do procesów roboczych Slave początek a_local i koniec b_local lokalnego przedziału całkowania dla danego procesu oraz liczbę N_local trapezów, z których należy policzyć całkę\n",
        "- zbiera wyniki cząstkowe z procesów, i wyświetla wynik całki"
      ],
      "metadata": {
        "id": "SKNadkuNvo9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "cat > pi-mpi.c << EOF\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "#include <math.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "double wzorfunkcji(double x)\n",
        "{\n",
        "  return pow(x, 2);\n",
        "}\n",
        "\n",
        "double wys(double a, double b, double N)\n",
        "{\n",
        "  return (b - a) / N;\n",
        "}\n",
        "\n",
        "int main(int argc,char **argv)\n",
        "{\n",
        "  int rank;\n",
        "  int num_proc;\n",
        "  int tag = 5;\n",
        "  MPI_Status status;\n",
        "\n",
        "  MPI_Init(&argc,&argv);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n",
        "\n",
        "  // Metoda Trapezów\n",
        "  double a = 2;\n",
        "  double b = 4;\n",
        "  double xi = 0;\n",
        "  double suma = 0;\n",
        "\n",
        "  double start = MPI_Wtime();\n",
        "\n",
        "  if(rank == num_proc - 1)\n",
        "  { \n",
        "    xi += (wzorfunkcji(a + (((double) rank / (double) num_proc) * (b - a))));\n",
        "    MPI_Send(&xi,1,MPI_DOUBLE,rank-1,tag,MPI_COMM_WORLD);\n",
        "\n",
        "  }\n",
        "  else if(rank != 0 && rank < num_proc - 1)\n",
        "  { \n",
        "    MPI_Recv(&xi,1,MPI_DOUBLE,rank+1,tag,MPI_COMM_WORLD,&status);\n",
        "    xi += (wzorfunkcji(a + (((double) rank / (double) num_proc) * (b - a))));           \n",
        "    MPI_Send(&xi,1,MPI_DOUBLE,rank-1,tag,MPI_COMM_WORLD);\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    MPI_Recv(&xi,1,MPI_DOUBLE,rank+1,tag,MPI_COMM_WORLD,&status);\n",
        "    suma = wys(a,b,num_proc) * (wzorfunkcji(a) / 2 + xi + wzorfunkcji(b)/2);\n",
        "    double end = MPI_Wtime();\n",
        "    printf(\"Wynik całki numerycznej metodą trapezów = %f \\n\", suma);\n",
        "    printf(\"Czas obliczen %f\",end - start);\n",
        "  }\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "EOF\n",
        "mpicc pi-mpi.c -lm && mpirun -n 4 --allow-run-as-root a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cceQ2X4v-E-",
        "outputId": "7928b560-eb63-4ed2-e149-80d5ef1c0ac9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wynik całki numerycznej metodą trapezów = 18.750000 \n",
            "Czas obliczen 0.000414"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zadanie 3** - Zaimplementuj program mnożenia macierzy A przez wektor B\n",
        "\n",
        "Proces 0, to proces Root, który zarządza obliczeniami:\n",
        "- inicjuje wartości macierzy A i wektora B\n",
        "- wysyła do procesów roboczych Slave fragmenty macierzy A i wektora B \n",
        "- zbiera wyniki cząstkowe z procesów, i wyświetla wynik całki"
      ],
      "metadata": {
        "id": "TdB2yaZur0YQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda8BM_3mBZ9",
        "outputId": "0f1314cc-719e-4ca3-face-dd2d2459b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Mnozenie macierzy za pomoca MPI \n",
            "\n",
            "Macierz A\n",
            "\n",
            "2\t9\t0\t4\t\n",
            "6\t2\t0\t6\t\n",
            "4\t4\t0\t3\t\n",
            "5\t0\t5\t5\t\n",
            "\n",
            "Macierz B\n",
            "\n",
            "1\t4\t4\t3\t\n",
            "7\t4\t0\t1\t\n",
            "1\t0\t4\t5\t\n",
            "5\t2\t3\t7\t\n",
            "\n",
            "Wynikowa macierz C = A * B:\n",
            "\n",
            "85\t52\t20\t43\t\n",
            "50\t44\t42\t62\t\n",
            "47\t38\t25\t37\t\n",
            "0\t0\t0\t0\t\n",
            "\n",
            "Czas obliczen 0.001096"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "cat > pi-mpi.c << EOF\n",
        "\n",
        "#include <stdlib.h> \n",
        "#include <stdio.h> \n",
        "#include <mpi.h> \n",
        "#include <time.h> \n",
        "#include <sys/time.h>\n",
        "\n",
        "#define N 4\n",
        "MPI_Status status;\n",
        "double A[N][N],B[N][N],C[N][N];\n",
        "\n",
        "int main(int argc, char **argv) \n",
        "{\n",
        "  int processCount, processId, slaveTaskCount, source, dest, rows, offset;\n",
        "  struct timeval start, stop;\n",
        "\n",
        "  // Inicjalizacja srodowiska MPI\n",
        "  MPI_Init(&argc, &argv);\n",
        "  // Uzyskanie numeru aktualnego procesu\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &processId);\n",
        "  // Uzyskanie liczby wszystkich procesów\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &processCount);\n",
        "\n",
        "  // Liczba procesów Slave mniejsza o 1 niz processCount\n",
        "  slaveTaskCount = processCount - 1;\n",
        "  // Przyjęto, ze proces 0 Root (Master)\n",
        "  if (processId == 0) \n",
        "  {\n",
        "    double start = MPI_Wtime();\n",
        "    // Inicjalizacja macierzy A i B\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i<N; i++) \n",
        "    {\n",
        "      for (int j = 0; j<N; j++) \n",
        "      { \n",
        "        A[i][j]= rand()%10; \n",
        "        B[i][j]= rand()%10;\n",
        "      }\n",
        "    }\n",
        "    // Wypisanie zawartości macierzy A i B\n",
        "    printf(\"\\n Mnozenie macierzy za pomoca MPI \\n\"); \n",
        "    printf(\"\\nMacierz A\\n\\n\");\n",
        "    for (int i = 0; i<N; i++) \n",
        "    {\n",
        "      for (int j = 0; j<N; j++) \n",
        "      {\n",
        "        printf(\"%.0f\\t\", A[i][j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\nMacierz B\\n\\n\"); \n",
        "    for (int i = 0; i<N; i++)\n",
        "    {\n",
        "      for (int j = 0; j<N; j++)\n",
        "      {\n",
        "        printf(\"%.0f\\t\", B[i][j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "    }\n",
        "    // Okreslene liczby wierzy macierzy A, która zostanie wysłana o każdego \n",
        "    // z procesów Slave\n",
        "    rows = N / slaveTaskCount;\n",
        "    // Zmienna offset określa aktualny pierwszy z wierszy do wysłania do\n",
        "    // aktualnego procesu Slave\n",
        "    offset = 0;\n",
        "    // Przygotowujemy wiersze i kolumny do wysłania do kolejnych procesów \n",
        "    // Slave o Id od 1 do slaveTaskCount, zostaną one przesłane wiadomością z tag 1\n",
        "    for(dest=1; dest <= slaveTaskCount; dest++)\n",
        "    {\n",
        "      // Przesylamy offset wzledem wiersza 0\n",
        "      MPI_Send(&offset, 1, MPI_INT, dest, 1, MPI_COMM_WORLD);\n",
        "      // Ile wierszy przesylamy\n",
        "      MPI_Send(&rows, 1, MPI_INT, dest, 1, MPI_COMM_WORLD);\n",
        "      // przesylamy wiersze macierzy A do procesow Slave\n",
        "      MPI_Send(&A[offset][0], rows*N, MPI_DOUBLE,dest,1, MPI_COMM_WORLD);\n",
        "      // Przesylamy kolumny macierzy B do procesow Slave\n",
        "      MPI_Send(&B, N*N, MPI_DOUBLE, dest, 1, MPI_COMM_WORLD);\n",
        "      // Modyfikujemy aktualną wartość zmiennej offset\n",
        "      offset = offset + rows; \n",
        "    }\n",
        "    // Proces Root czeka aż procesy Slave obliczą cząstkowe iloczyny wierszy \n",
        "    // i kolumn z macierzy A i B, wyniki zostaną odebrane z wiadomościach z tag 2\n",
        "    for (int i = 1; i <= slaveTaskCount; i++)\n",
        "    {\n",
        "      source = i;\n",
        "      // Proces Root otrzymuje offset od aktualnego procesu Slave\n",
        "      MPI_Recv(&offset, 1, MPI_INT, source, 2, MPI_COMM_WORLD, &status);\n",
        "      // Proces Root otrzymuje liczbę wierszy, którą otrzyma od aktualnego procesu Slave\n",
        "      MPI_Recv(&rows, 1, MPI_INT, source, 2, MPI_COMM_WORLD, &status);\n",
        "      //Otrzymanie danych cząstkowych z mnożenia A*B do C \n",
        "      MPI_Recv(&C[offset][0], rows*N, MPI_DOUBLE, source, 2, MPI_COMM_WORLD, &status);\n",
        "    }\n",
        "    // Wypisanie macierzy wynikowej C\n",
        "    printf(\"\\nWynikowa macierz C = A * B:\\n\\n\"); \n",
        "    for (int i = 0; i<N; i++) \n",
        "    {\n",
        "      for (int j = 0; j<N; j++)\n",
        "      {\n",
        "        printf(\"%.0f\\t\", C[i][j]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    double end = MPI_Wtime();\n",
        "    printf(\"Czas obliczen %f\",end - start);\n",
        "  } //Koniec kodu procesu Root\n",
        "  // Kod do wykonania przez procesy Slave\n",
        "  if (processId > 0)\n",
        "  {\n",
        "    // Podanie procesom Slave numeru procesu Root\n",
        "    source = 0;\n",
        "    // Procesy Slave czekają na wiadomości z tag 1 od procesu Root \n",
        "    // Każdy z procesów Slave wykonuje oddzielnie następujący kod\n",
        "    // Procesy Slave otrzymują wartość offsetu od procesu Root\n",
        "    MPI_Recv(&offset, 1, MPI_INT, source, 1, MPI_COMM_WORLD, &status);\n",
        "    // Procesy Slave otrzymują liczbę wierszy, która zostanie przesłana od procesu Root\n",
        "    MPI_Recv(&rows, 1, MPI_INT, source, 1, MPI_COMM_WORLD, &status);\n",
        "    // Procesy Slave otrzymują fragmenty macierzy A od procesu Root \n",
        "    MPI_Recv(&A, rows*N, MPI_DOUBLE, source, 1, MPI_COMM_WORLD, &status);\n",
        "    // Procesy Slave otrzymują fragmenty macierzy B od procesu Root\n",
        "    MPI_Recv(&B, N*N, MPI_DOUBLE, source, 1, MPI_COMM_WORLD, &status);\n",
        "    // Mnożenie macierz y\n",
        "    for (int k = 0; k<N; k++)\n",
        "    {\n",
        "      for (int i = 0; i<rows; i++)\n",
        "      {\n",
        "        C[i][k] = 0.0;\n",
        "        // Element A[i][j] mnożony przez B[j][k] \n",
        "        for (int j = 0; j<N; j++)\n",
        "        {\n",
        "          C[i][k] = C[i][k] + A[i][j] * B[j][k]; \n",
        "        }      \n",
        "      }\n",
        "    }\n",
        "    // Wyniki cząstkowe są wysyłane z procesów Slave do procesu Root wiadomościami z tag 2\n",
        "    // Offset zostanie wysłany do Root\n",
        "    MPI_Send(&offset, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);\n",
        "    // Liczba wierszy zostanie wysłana do Root\n",
        "    MPI_Send(&rows, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);\n",
        "    // Wyznaczony fragment macierzy C zostanie wysłany do Root\n",
        "    MPI_Send(&C, rows*N, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD);\n",
        "  }\n",
        "  MPI_Finalize();\n",
        "}\n",
        "\n",
        "EOF\n",
        "mpicc pi-mpi.c && mpirun -n 4 --allow-run-as-root a.out"
      ]
    }
  ]
}